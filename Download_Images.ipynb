{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MiroPol21/dspracticum2025/blob/main/lesson02/Own_images_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5j8Ih8JeWh8"
   },
   "source": [
    "Let's build a simple neural network to classify images.\n",
    "\n",
    "**1. Install ddgs package**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "umrk5Tygla-3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ddgs in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (9.6.0)\n",
      "Requirement already satisfied: click>=8.1.8 in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from ddgs) (8.1.8)\n",
      "Requirement already satisfied: primp>=0.15.0 in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from ddgs) (0.15.0)\n",
      "Requirement already satisfied: lxml>=6.0.0 in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from ddgs) (6.0.2)\n",
      "Requirement already satisfied: httpx>=0.28.1 in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.28.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from click>=8.1.8->ddgs) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\dhalm\\appdata\\roaming\\python\\python310\\site-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\dhalm\\appdata\\roaming\\python\\python310\\site-packages (from httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from httpcore==1.*->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.3.0)\n",
      "Requirement already satisfied: brotli in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.1.0)\n",
      "Requirement already satisfied: socksio==1.* in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.0.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from h2<5,>=3->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.1.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (1.3.1)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in c:\\users\\dhalm\\anaconda3\\envs\\py31013\\lib\\site-packages (from anyio->httpx>=0.28.1->httpx[brotli,http2,socks]>=0.28.1->ddgs) (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "# run only once\n",
    "!pip install ddgs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgpXcTFsl4K4"
   },
   "source": [
    "**2. Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MBQVdt1S-MT9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from ddgs import DDGS\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDw2Gl-lmJFj"
   },
   "source": [
    "**3. Import images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "nSUWiuxOlkVW"
   },
   "outputs": [],
   "source": [
    "def search_images(keyword, max_results=10):\n",
    "    with DDGS() as ddgs:\n",
    "        images = ddgs.images(\n",
    "            keyword,\n",
    "            max_results=max_results\n",
    "        )\n",
    "        return [img['image'] for img in images]\n",
    "\n",
    "def search_more_images(base_keyword, target_results=200):\n",
    "    \"\"\"\n",
    "    Search for more images using multiple keyword variations\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    all_urls = []\n",
    "    \n",
    "    # Different search term variations to get more results\n",
    "    search_terms = [\n",
    "        base_keyword,\n",
    "        f\"{base_keyword} photo\",\n",
    "        f\"{base_keyword} picture\", \n",
    "        f\"{base_keyword} image\",\n",
    "        f\"cute {base_keyword}\",\n",
    "        f\"{base_keyword} animal\",\n",
    "    ]\n",
    "    \n",
    "    print(f\"Searching for {base_keyword} images...\")\n",
    "    \n",
    "    for i, term in enumerate(search_terms):\n",
    "        if len(all_urls) >= target_results:\n",
    "            break\n",
    "            \n",
    "        try:\n",
    "            # Search for images with this term\n",
    "            urls = search_images(term, max_results=100)\n",
    "            \n",
    "            # Remove duplicates\n",
    "            new_urls = [url for url in urls if url not in all_urls]\n",
    "            all_urls.extend(new_urls)\n",
    "            \n",
    "            print(f\"  Search {i+1}/8: '{term}' -> {len(urls)} found, {len(new_urls)} new (total: {len(all_urls)})\")\n",
    "            \n",
    "            # Add small delay to avoid rate limiting\n",
    "            if i < len(search_terms) - 1:  # Don't sleep after the last search\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  Error searching for '{term}': {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"Final result for '{base_keyword}': {len(all_urls)} images\")\n",
    "    return all_urls[:target_results]  # Return only the requested amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SGJJDjh6nQFU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for cat images...\n",
      "  Search 1/8: 'cat' -> 100 found, 100 new (total: 100)\n",
      "  Search 2/8: 'cat photo' -> 100 found, 99 new (total: 199)\n",
      "  Search 3/8: 'cat picture' -> 100 found, 93 new (total: 292)\n",
      "Final result for 'cat': 292 images\n",
      "Searching for dog images...\n",
      "  Search 1/8: 'dog' -> 100 found, 100 new (total: 100)\n",
      "  Search 2/8: 'dog photo' -> 100 found, 98 new (total: 198)\n",
      "  Search 3/8: 'dog picture' -> 100 found, 88 new (total: 286)\n",
      "Final result for 'dog': 286 images\n",
      "Searching for rabbit images...\n",
      "  Search 1/8: 'rabbit' -> 100 found, 100 new (total: 100)\n",
      "  Search 2/8: 'rabbit photo' -> 100 found, 95 new (total: 195)\n",
      "  Search 3/8: 'rabbit picture' -> 100 found, 86 new (total: 281)\n",
      "Final result for 'rabbit': 281 images\n",
      "Searching for capybara images...\n",
      "  Search 1/8: 'capybara' -> 100 found, 100 new (total: 100)\n",
      "  Search 2/8: 'capybara photo' -> 100 found, 86 new (total: 186)\n",
      "  Search 3/8: 'capybara picture' -> 100 found, 72 new (total: 258)\n",
      "Final result for 'capybara': 258 images\n",
      "Searching for owl images...\n",
      "  Search 1/8: 'owl' -> 100 found, 100 new (total: 100)\n",
      "  Search 2/8: 'owl photo' -> 99 found, 92 new (total: 192)\n",
      "  Search 3/8: 'owl picture' -> 100 found, 60 new (total: 252)\n",
      "Final result for 'owl': 252 images\n"
     ]
    }
   ],
   "source": [
    "keywords = ['cat', 'dog', 'rabbit', 'capybara', 'owl']\n",
    "\n",
    "# Use the enhanced search function to get 200 images per category\n",
    "image_urls_list = []\n",
    "for keyword in keywords:\n",
    "    urls = search_more_images(keyword, 200)\n",
    "    image_urls_list.append(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images found per category:\n",
      "  cat: 200 images\n",
      "  dog: 200 images\n",
      "  rabbit: 200 images\n",
      "  capybara: 200 images\n",
      "  owl: 200 images\n"
     ]
    }
   ],
   "source": [
    "print(\"Images found per category:\")\n",
    "for i, keyword in enumerate(keywords):\n",
    "    print(f\"  {keyword}: {len(image_urls_list[i])} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1FmHRWWoTak"
   },
   "source": [
    "**4. Download Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_o72Se5noWCT"
   },
   "outputs": [],
   "source": [
    "def download_image(url, folder, custom_name=None, verbose=True):\n",
    "    # Create the folder if it doesn't exist\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    # Get the filename from the URL or use the custom name\n",
    "    if custom_name:\n",
    "        filename = custom_name\n",
    "    else:\n",
    "        filename = os.path.basename(urlparse(url).path)\n",
    "        if not filename:\n",
    "            filename = 'image.jpg'  # Default filename if none is found in the URL\n",
    "\n",
    "    # Ensure the filename has an extension\n",
    "    if not os.path.splitext(filename)[1]:\n",
    "        filename += '.jpg'\n",
    "\n",
    "    filepath = os.path.join(folder, filename)\n",
    "\n",
    "    # If the file already exists, append a number to make it unique\n",
    "    base, extension = os.path.splitext(filepath)\n",
    "    counter = 1\n",
    "    while os.path.exists(filepath):\n",
    "        filepath = f\"{base}_{counter}{extension}\"\n",
    "        counter += 1\n",
    "\n",
    "    try:\n",
    "        # Send a GET request to the URL with a timeout of 10 seconds\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "\n",
    "        # Check if the content type is an image\n",
    "        content_type = response.headers.get('content-type', '')\n",
    "        if not content_type.startswith('image'):\n",
    "            if verbose:\n",
    "                warnings.warn(f\"The URL does not point to an image. Content-Type: {content_type}\")\n",
    "            return False\n",
    "\n",
    "        # Write the image content to the file\n",
    "        with open(filepath, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Image successfully downloaded: {filepath}\")\n",
    "        return True\n",
    "\n",
    "    except requests.exceptions.Timeout:\n",
    "        if verbose:\n",
    "            warnings.warn(f\"Download timed out for URL: {url}\")\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        if verbose:\n",
    "            warnings.warn(f\"HTTP error occurred: {e}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        if verbose:\n",
    "            warnings.warn(f\"An error occurred while downloading the image: {e}\")\n",
    "    except IOError as e:\n",
    "        if verbose:\n",
    "            warnings.warn(f\"An error occurred while writing the file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wz3eD-t5oXcb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca1fc8ba0c8497d8c610c6b69a94532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e090e1b2f3440baed79df60d142493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "520d9f81de934ecdaaf7acf02b31ff39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b4df0f0ea746b5bd26187f654b67ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed88d0bf4e3743e68e6a793f6a2643eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b37142ed5e641619e004d62b9e52b49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i, image_urls in enumerate(tqdm(image_urls_list)):\n",
    "    for j, url in enumerate(tqdm(image_urls)):\n",
    "        download_image(url, f\"./dataset/{keywords[i]}/\", f'image{j}.jpg', verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6e143603"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking directory: ./dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "def remove_invalid_images(directory):\n",
    "    \"\"\"\n",
    "    Removes files from a directory that are not valid images.\n",
    "    \"\"\"\n",
    "    print(f\"Checking directory: {directory}\")\n",
    "    for subdir, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            filepath = os.path.join(subdir, file)\n",
    "            try:\n",
    "                with warnings.catch_warnings():\n",
    "                    warnings.simplefilter(\"ignore\", Image.DecompressionBombWarning)\n",
    "                    img = Image.open(filepath)\n",
    "                    img.verify() # Verify that it is an image\n",
    "            except Exception:\n",
    "                # If verification fails, delete the file\n",
    "                if os.path.exists(filepath):\n",
    "                    print(f\"Removing invalid image: {filepath}\")\n",
    "                    os.remove(filepath)\n",
    "\n",
    "remove_invalid_images('./dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Split dataset into train/test folders**\n",
    "\n",
    "According to the homework requirements, we need to organize our dataset into the proper folder structure with 75% train and 25% test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found classes: ['capybara', 'cat', 'dog', 'owl', 'rabbit']\n",
      "Class 'capybara': 175 images\n",
      "  - Train: 131 images\n",
      "  - Test: 44 images\n",
      "Class 'cat': 168 images\n",
      "  - Train: 126 images\n",
      "  - Test: 42 images\n",
      "Class 'dog': 174 images\n",
      "  - Train: 130 images\n",
      "  - Test: 44 images\n",
      "Class 'owl': 157 images\n",
      "  - Train: 117 images\n",
      "  - Test: 40 images\n",
      "Class 'rabbit': 177 images\n",
      "  - Train: 132 images\n",
      "  - Test: 45 images\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "\n",
    "def split_dataset_to_folders(source_dir='./dataset', target_dir='./animals_dataset', test_size=0.25, random_state=42):\n",
    "    \"\"\"\n",
    "    Split images from source_dir into train/test folders with the required structure.\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Current dataset directory with class folders\n",
    "        target_dir: New directory to create with train/test split\n",
    "        test_size: Proportion of images to use for testing (0.25 = 25%)\n",
    "        random_state: Random seed for reproducible splits\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create target directory structure\n",
    "    train_dir = os.path.join(target_dir, 'train')\n",
    "    test_dir = os.path.join(target_dir, 'test')\n",
    "    \n",
    "    # Get all class folders (cat, dog, rabbit, etc.)\n",
    "    class_folders = [d for d in os.listdir(source_dir) \n",
    "                    if os.path.isdir(os.path.join(source_dir, d))]\n",
    "    \n",
    "    print(f\"Found classes: {class_folders}\")\n",
    "    \n",
    "    for class_name in class_folders:\n",
    "        # Create class directories in train and test\n",
    "        os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
    "        os.makedirs(os.path.join(test_dir, class_name), exist_ok=True)\n",
    "        \n",
    "        # Get all image files for this class\n",
    "        class_path = os.path.join(source_dir, class_name)\n",
    "        image_files = glob.glob(os.path.join(class_path, '*.jpg')) + \\\n",
    "                     glob.glob(os.path.join(class_path, '*.png')) + \\\n",
    "                     glob.glob(os.path.join(class_path, '*.jpeg'))\n",
    "        \n",
    "        print(f\"Class '{class_name}': {len(image_files)} images\")\n",
    "        \n",
    "        if len(image_files) == 0:\n",
    "            print(f\"Warning: No images found for class {class_name}\")\n",
    "            continue\n",
    "            \n",
    "        # Split the file paths\n",
    "        train_files, test_files = train_test_split(\n",
    "            image_files, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        # Copy files to train directory\n",
    "        for file_path in train_files:\n",
    "            filename = os.path.basename(file_path)\n",
    "            target_path = os.path.join(train_dir, class_name, filename)\n",
    "            shutil.copy2(file_path, target_path)\n",
    "        \n",
    "        # Copy files to test directory  \n",
    "        for file_path in test_files:\n",
    "            filename = os.path.basename(file_path)\n",
    "            target_path = os.path.join(test_dir, class_name, filename)\n",
    "            shutil.copy2(file_path, target_path)\n",
    "            \n",
    "        print(f\"  - Train: {len(train_files)} images\")\n",
    "        print(f\"  - Test: {len(test_files)} images\")\n",
    "\n",
    "# Run the split\n",
    "split_dataset_to_folders()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py31013",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
